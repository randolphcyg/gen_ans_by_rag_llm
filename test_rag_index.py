import logging
import os
import requests
import torch
import json
import time
import hashlib
from typing import List, Dict, Any
from sentence_transformers import CrossEncoder
from pymilvus import MilvusClient, DataType
from langfuse import Langfuse
from langfuse import observe

# ===================== 1. å…¨å±€é…ç½® =====================

# --- Langfuse é…ç½® ---
LANGFUSE_SECRET_KEY = "sk-lf-93542e4b-15ef-4a50-8719-0a12fbc42a8b"
LANGFUSE_PUBLIC_KEY = "pk-lf-f7f639cf-2585-4578-9404-26dec6b91626"
LANGFUSE_BASE_URL = "http://localhost:3100"

# --- æ ¸å¿ƒæœåŠ¡åœ°å€ ---
MILVUS_URI = "http://localhost:19530"
OLLAMA_HOST = "http://localhost:11434"

# --- å¯¹æ¯”é…ç½® ---
COLLECTION_HIERARCHICAL = "Vector_index_0804549e_ed61_4f22_9f94_16176bb0cede_Node"
COLLECTION_GENERAL      = "Vector_index_19191596_0e1f_492c_ab31_15e11501cec4_Node"

# --- æ¨¡å‹é…ç½® ---
EMBED_MODEL = "bge-m3:latest"
LLM_MODEL = "qwen2.5-coder:3b"
RERANKER_MODEL_NAME = 'BAAI/bge-reranker-base'

# --- ç®—æ³•å‚æ•° ---
RETRIEVE_TOP_K = 30
RERANK_TOP_K = 8
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LOCAL_RERANKER_PATH = os.path.join(BASE_DIR, "models", "bge-reranker-base")

# ===================== 2. åˆå§‹åŒ–æœåŠ¡ =====================
# æå‰é…ç½®æ—¥å¿—ï¼Œç¡®ä¿ç”¨æˆ·èƒ½çœ‹åˆ°å¯åŠ¨è¿‡ç¨‹
logging.basicConfig(level=logging.ERROR, format="%(asctime)s - %(levelname)s - %(message)s")
print("â³ æ­£åœ¨åˆå§‹åŒ– AI å¼•æ“ (åŠ è½½ PyTorch & CUDA)...")

langfuse = Langfuse(
    secret_key=LANGFUSE_SECRET_KEY,
    public_key=LANGFUSE_PUBLIC_KEY,
    host=LANGFUSE_BASE_URL,
    debug=False,
    timeout=3
)

class ZeekRAGComparisonAssistant:
    def __init__(self):
        self.session = requests.Session()
        self.schemas = {}

        try:
            # 1. åˆå§‹åŒ– Milvus
            self.milvus_client = MilvusClient(uri=MILVUS_URI)
            self._register_collection(COLLECTION_HIERARCHICAL, "çˆ¶å­ç´¢å¼•")
            self._register_collection(COLLECTION_GENERAL, "é€šç”¨ç´¢å¼•")

            # 2. ğŸš€ å¼ºåˆ¶ GPU æ£€æŸ¥
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ğŸ–¥ï¸ è®¡ç®—è®¾å¤‡æ£€æµ‹: {self.device.upper()}")

            if self.device == 'cpu':
                print("âš ï¸ [ä¸¥é‡è­¦å‘Š] æœªæ£€æµ‹åˆ° GPUï¼å»ºè®®å®‰è£… CUDA ç‰ˆ PyTorchã€‚")

            model_path = LOCAL_RERANKER_PATH if os.path.exists(LOCAL_RERANKER_PATH) else RERANKER_MODEL_NAME

            # 3. åŠ è½½ Reranker
            self.reranker = CrossEncoder(
                model_path,
                device=self.device,
                # ä¿®å¤ Warning: ä½¿ç”¨ model_kwargs æ›¿ä»£ automodel_args
                model_kwargs={"dtype": "auto"}
            )
            print(f"âœ… Reranker æ¨¡å‹åŠ è½½å®Œæˆ")

        except Exception as e:
            logging.error(f"âŒ åˆå§‹åŒ–ä¸¥é‡å¤±è´¥: {e}")
            raise

    def _register_collection(self, collection_name, label):
        """æ³¨å†Œå¹¶æ¢æµ‹é›†åˆ Schema"""
        try:
            res = self.milvus_client.describe_collection(collection_name)
        except Exception as e:
            print(f"âŒ æ— æ³•è¿æ¥é›†åˆ {collection_name}: {e}")
            return

        schema_info = {"vector_field": "vector", "text_field": "text", "meta_field": "meta", "label": label}

        for field in res.get('fields', []):
            if field['type'] == DataType.FLOAT_VECTOR: schema_info["vector_field"] = field['name']
            if field['type'] == DataType.VARCHAR and field['name'] in ['text', 'page_content', 'raw_content']: schema_info["text_field"] = field['name']
            if field['type'] == DataType.JSON: schema_info["meta_field"] = field['name']

        self.schemas[collection_name] = schema_info
        print(f"ğŸ” [{label}] Schema: Vector='{schema_info['vector_field']}', Text='{schema_info['text_field']}'")

    def _route_prompt(self, query: str) -> str:
        q = query.lower()
        if any(k in q for k in ["script", "code", "write", "function", "generate", "è„šæœ¬", "ä»£ç ", "ç¼–å†™"]):
            return "zeek_coder"
        if any(k in q for k in ["error", "fail", "empty", "why", "debug", "fix", "æŠ¥é”™", "ä¸ºç©º", "æ’é”™"]):
            return "zeek_debugger"
        return "zeek_explainer"

    @observe(name="RAG-æ£€ç´¢é˜¶æ®µ")
    def retrieve(self, query: str, collection_name: str):
        schema = self.schemas.get(collection_name)
        if not schema: return []

        # Embedding
        try:
            resp = self.session.post(
                f"{OLLAMA_HOST}/api/embed",
                json={"model": EMBED_MODEL, "input": [query]},
                timeout=30
            )
            query_vecs = resp.json().get("embeddings", [])
        except Exception as e:
            logging.error(f"Embedding å¤±è´¥: {e}")
            return []

        # Milvus Search
        v_hits = []
        if query_vecs:
            try:
                res = self.milvus_client.search(
                    collection_name=collection_name,
                    data=query_vecs,
                    anns_field=schema["vector_field"],
                    limit=RETRIEVE_TOP_K,
                    output_fields=[schema["text_field"], schema["meta_field"]],
                    search_params={"metric_type": "IP", "params": {"nprobe": 10}}
                )

                for hits in res:
                    for hit in hits:
                        entity = hit["entity"]
                        adapted_hit = {
                            "score": hit["distance"],
                            "raw_content": entity.get(schema["text_field"], "")
                        }
                        v_hits.append(adapted_hit)
            except Exception as e:
                logging.error(f"Milvus æœç´¢å¤±è´¥: {e}")

        return v_hits

    @observe(name="RAG-é‡æ’é˜¶æ®µ")
    def rerank(self, query: str, hits: List[Dict]):
        if not hits: return []

        pairs = [[query[:512], h["raw_content"][:1024]] for h in hits]
        try:
            bge_scores = self.reranker.predict(pairs, batch_size=16)
        except Exception as e:
            return hits[:3]

        for i, hit in enumerate(hits):
            hit["final_score"] = float(bge_scores[i])

        sorted_hits = sorted(hits, key=lambda x: x["final_score"], reverse=True)

        unique_hits = []
        seen_content = set()

        for h in sorted_hits:
            content_sig = hashlib.md5(h["raw_content"][:100].encode('utf-8')).hexdigest()
            if content_sig not in seen_content:
                unique_hits.append(h)
                seen_content.add(content_sig)
            if len(unique_hits) >= RERANK_TOP_K: break

        return unique_hits

    @observe(name="LLM-ç”Ÿæˆå›ç­”")
    def ask_llm(self, query: str, chunks: List[Dict]):
        if not chunks: return "æœªæ‰¾åˆ°ç›¸å…³ Zeek æ–‡æ¡£ï¼Œæ— æ³•å›ç­”ã€‚"

        context_str = ""
        for i, c in enumerate(chunks):
            clean_text = c['raw_content'].strip()
            context_str += f"### Reference [{i+1}]:\n{clean_text}\n\n"

        prompt_name = self._route_prompt(query)
        final_prompt = ""

        try:
            lf_prompt = langfuse.get_prompt(prompt_name)
            compiled = lf_prompt.compile(context=context_str, query=query)
            for msg in compiled:
                role_prefix = "SYSTEM" if msg['role'] == 'system' else "USER"
                final_prompt += f"{role_prefix}: {msg['content']}\n\n"

        except Exception as e:
            # å…œåº• Prompt
            if prompt_name == "zeek_coder":
                sys_msg = "You are a Zeek Scripting Expert. Write code based STRICTLY on the context. Use modern Zeek syntax."
            else:
                sys_msg = "You are a Zeek Expert. Answer based on the context."

            final_prompt = f"SYSTEM: {sys_msg}\n\nUSER: Context:\n{context_str}\n\nQuestion: {query}\nAnswer:"

        try:
            r = self.session.post(f"{OLLAMA_HOST}/api/generate", json={
                "model": LLM_MODEL,
                "prompt": final_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "num_ctx": 4096
                }
            })
            return r.json().get("response", "LLM ç”Ÿæˆä¸ºç©º")
        except Exception as e:
            return f"LLM è°ƒç”¨é”™è¯¯: {e}"

    def run_comparison(self, query: str):
        print(f"\n{'='*20} ğŸŸ¢ æµ‹è¯•é—®é¢˜: {query} {'='*20}")
        prompt_type = self._route_prompt(query)
        print(f"ğŸ¨ è·¯ç”± Prompt ç±»å‹: {prompt_type}")

        targets = [
            (COLLECTION_HIERARCHICAL, "ğŸ‘¨â€ğŸ‘¦ çˆ¶å­ç´¢å¼•"),
            (COLLECTION_GENERAL,      "ğŸ“„ é€šç”¨ç´¢å¼•")
        ]

        for col_id, label in targets:
            print(f"\n>>> æ­£åœ¨æµ‹è¯•: {label} ...")
            start_t = time.time()

            try:
                hits = self.retrieve(query, col_id)
                top_hits = self.rerank(query, hits)
                ans = self.ask_llm(query, top_hits)
                cost = time.time() - start_t

                print(f"â±ï¸ æ€»è€—æ—¶: {cost:.2f}s")
                print(f"ğŸ¤– å›ç­”:\n{ans.strip()}")
                print(f"\nğŸ“š å¼•ç”¨ç‰‡æ®µ (Top 3):")
                for i, h in enumerate(top_hits[:3]):
                    preview = h['raw_content'][:100].replace('\n', ' ')
                    print(f"   [{i+1}] Score: {h['final_score']:.4f} | {preview}...")

            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")

            print("-" * 50)

if __name__ == "__main__":
    assistant = ZeekRAGComparisonAssistant()

    test_queries = [
        "what is zeek?",
        "Explain the meaning of the `history` field string 'ShADadFf' in `conn.log`.",
        "Write a script to handle `ssh_auth_successful` event. Do I need to load any module?",
        "Why is my `notice.log` empty even though I see attacks in `conn.log`?"
    ]

    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œ {len(test_queries)} ä¸ªæµ‹è¯•ç”¨ä¾‹...")

    for query in test_queries:
        assistant.run_comparison(query)
        langfuse.flush()
        time.sleep(1)