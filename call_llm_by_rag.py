import logging
import os
import requests
import torch
import jieba
from typing import List, Dict
from sentence_transformers import CrossEncoder
from pymilvus import MilvusClient
from stopwords import StopWordsManager
from langfuse import Langfuse
from langfuse import observe

# ===================== 1. é…ç½®ç®¡ç† =====================
# langfuseé…ç½®
LANGFUSE_SECRET_KEY = "sk-lf-ab1f9c14-4b8e-4d76-8533-52a2985fb4e3"
LANGFUSE_PUBLIC_KEY = "pk-lf-76c0c3e9-90c6-45af-b564-b9ed7052daf4"
LANGFUSE_BASE_URL = "http://localhost:3100"

# æ ¸å¿ƒæœåŠ¡åœ°å€
MILVUS_URI = "http://localhost:19530"
OLLAMA_HOST = "http://localhost:11434"

# æ¨¡å‹é…ç½®
COLLECTION_NAME = "zeek_rag_v8_0_4"
EMBED_MODEL = "bge-m3:latest"
LLM_MODEL = "qwen2.5-coder:3b"
RERANKER_MODEL_NAME = 'BAAI/bge-reranker-base'

# ç®—æ³•å‚æ•°
RETRIEVE_TOP_K = 50
RERANK_TOP_K = 3
RRF_K = 60
MAX_CONTEXT_CHARS = 3500
SCORE_THRESHOLD = -8.0

# è‡ªåŠ¨æ¨å¯¼æœ¬åœ°æ¨¡å‹è·¯å¾„ (å‡è®¾æ¨¡å‹åœ¨å½“å‰è„šæœ¬åŒçº§çš„ models ç›®å½•ä¸‹)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LOCAL_RERANKER_PATH = os.path.join(BASE_DIR, "models", "bge-reranker-base")

# ===================== 2. åˆå§‹åŒ–æœåŠ¡ =====================
langfuse = Langfuse(
    secret_key=LANGFUSE_SECRET_KEY,
    public_key=LANGFUSE_PUBLIC_KEY,
    host=LANGFUSE_BASE_URL,
    debug=False
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

class ZeekRAGAssistant:
    def __init__(self):
        self.session = requests.Session()

        self.stop_words_manager = StopWordsManager()
        jieba.initialize()  # æ˜¾å¼åˆå§‹åŒ–

        try:
            self.milvus_client = MilvusClient(uri=MILVUS_URI)

            device = "cuda" if torch.cuda.is_available() else "cpu"
            model_path = RERANKER_MODEL_NAME

            # æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å­˜åœ¨
            if os.path.exists(LOCAL_RERANKER_PATH):
                logging.info(f"ğŸ“‚ å‘ç°æœ¬åœ°æ¨¡å‹ï¼Œå¯ç”¨ç¦»çº¿æ¨¡å¼: {LOCAL_RERANKER_PATH}")
                os.environ["TRANSFORMERS_OFFLINE"] = "1"
                os.environ["HF_HUB_OFFLINE"] = "1"
                model_path = LOCAL_RERANKER_PATH
            else:
                logging.info(f"ğŸŒ æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†å°è¯•åœ¨çº¿ä¸‹è½½: {RERANKER_MODEL_NAME}")

            # åŠ è½½é‡æ’æ¨¡å‹
            self.reranker = CrossEncoder(model_path, device=device)
            logging.info(f"âœ… åŠ©æ‰‹åˆå§‹åŒ–å®Œæˆ | Reranker: {device} | LLM: {LLM_MODEL}")

            # é¢„çƒ­ Ollama æ¨¡å‹
            logging.info("ğŸ”¥ æ­£åœ¨é¢„çƒ­ Ollama æ¨¡å‹...")
            self.session.post(
                f"{OLLAMA_HOST}/api/generate",
                json={"model": LLM_MODEL, "prompt": "hi", "stream": False},
                timeout=1 # è¿™é‡Œè¶…æ—¶æ— æ‰€è°“ï¼Œåªè¦è§¦å‘åŠ è½½å°±è¡Œ
            )

        except Exception as e:
            logging.error(f"âŒ åˆå§‹åŒ–ä¸¥é‡å¤±è´¥: {e}")
            raise

    def get_embedding(self, text: str):
        """è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤ºï¼ˆå¢åŠ å¼‚å¸¸å¤„ç†ï¼‰"""
        try:
            resp = self.session.post(
                f"{OLLAMA_HOST}/api/embed",
                json={"model": EMBED_MODEL, "input": [text]},
                timeout=10
            )
            return resp.json()["embeddings"][0]
        except: return None

    def generate_multi_queries(self, original_query: str) -> List[str]:
        """
        æœ€ä½³å®è·µï¼šåˆ©ç”¨ LLM ç”Ÿæˆ 3 ä¸ªä¸åŒç»´åº¦çš„æœç´¢è¯
        1. åŸå§‹è¯
        2. ä¸“å®¶æœ¯è¯­è¯
        3. å‡è®¾æ€§ä»£ç ç‰‡æ®µ (HyDE æ€æƒ³)
        """
        # ç®€å•é—®é¢˜ä¸æ¶ˆè€— Token
        if len(original_query) < 5: return [original_query]

        try:
            # ä» Langfuse åŠ è½½ Prompt
            langfuse_prompt = langfuse.get_prompt("zeek_query_expansion")
            compiled_prompt = langfuse_prompt.compile(query=original_query)

            # è¿™é‡Œå¯ä»¥ç”¨æ›´å°çš„æ¨¡å‹ (å¦‚ qwen:0.5b æˆ– 1.5b) æ¥é™ä½å»¶è¿Ÿ
            resp = self.session.post(
                f"{OLLAMA_HOST}/api/generate",
                json={
                    "model": LLM_MODEL,
                    "prompt": compiled_prompt,
                    "stream": False,
                    "options": {"temperature": 0.5} # ç¨å¾®æœ‰ç‚¹åˆ›é€ åŠ›
                },
                timeout=20
            )
            text = resp.json().get("response", "").strip()

            # è§£æ LLM è¾“å‡ºçš„ 3 è¡Œæ–‡æœ¬
            queries = [line.strip() for line in text.split('\n') if line.strip()]

            # å…œåº•ï¼šå¦‚æœ LLM æ ¼å¼ä¹±äº†ï¼Œè‡³å°‘ä¿åº•åŸå§‹é—®é¢˜
            if not queries: queries = [original_query]

            logging.info(f"ğŸš€ å¤šè·¯æŸ¥è¯¢ç”Ÿæˆ: {queries}")
            return queries

        except Exception as e:
            logging.warning(f"æŸ¥è¯¢æ‰©å±•å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æŸ¥è¯¢: {e}")
            return [original_query]

    @observe(name="RAG-æ£€ç´¢é˜¶æ®µ")
    def retrieve(self, query: str):
        # 1. --- æ ¸å¿ƒå‡çº§ï¼šè·å–å¤šè·¯æŸ¥è¯¢ ---
        search_queries = self.generate_multi_queries(query)

        # 2. --- æ ¸å¿ƒå‡çº§ï¼šæ‰¹é‡å‘é‡åŒ– (Batch Embedding) ---
        # Ollama æ”¯æŒ batch inputï¼Œä¸€æ¬¡ç½‘ç»œè¯·æ±‚æ‹¿å› 3 ä¸ªå‘é‡ï¼Œæ¯”å¾ªç¯å¿«
        query_vecs = []
        try:
            resp = self.session.post(
                f"{OLLAMA_HOST}/api/embed",
                json={"model": EMBED_MODEL, "input": search_queries},
                timeout=10
            )
            query_vecs = resp.json().get("embeddings", [])
        except Exception as e:
            logging.error(f"Embedding å¤±è´¥: {e}")

        # 3. --- æ ¸å¿ƒå‡çº§ï¼šMilvus æ‰¹é‡æ£€ç´¢ ---
        v_hits = []
        if query_vecs:
            try:
                # search_requests å¯ä»¥å¹¶è¡Œæœç´¢
                res = self.milvus_client.search(
                    collection_name=COLLECTION_NAME,
                    data=query_vecs,  # ä¼ å…¥å¤šä¸ªå‘é‡
                    limit=RETRIEVE_TOP_K // 2, # æ¯ä¸ªå‘é‡å°‘å–ç‚¹ï¼Œåæ­£è¦åˆå¹¶
                    output_fields=["doc_title", "section_title", "raw_content", "content_type"]
                )
                # å±•å¹³ç»“æœ (res æ˜¯ä¸€ä¸ªäºŒç»´åˆ—è¡¨: [ [query1_hits], [query2_hits] ])
                for hits in res:
                    for hit in hits:
                        v_hits.append(hit["entity"])
            except Exception as e:
                logging.error(f"Milvus æœç´¢å¤±è´¥: {e}")

        # 4. å…³é”®è¯æ£€ç´¢ (å¯¹æ‰€æœ‰æ‰©å±•è¯éƒ½åšä¸€éå…³é”®è¯åŒ¹é…)
        k_hits = []
        # å°†æ‰€æœ‰æ‰©å±•è¯åˆå¹¶æˆä¸€ä¸ªå¤§çš„å…³é”®è¯æ± ï¼Œå»é‡
        all_keywords = set()
        for q in search_queries:
            kws = self.stop_words_manager.filter_stop_words(q)
            all_keywords.update(kws if kws else [q])

        # æ„é€ å¤æ‚çš„ OR æŸ¥è¯¢
        filter_exprs = []
        for kw in list(all_keywords)[:8]: # é™åˆ¶æ•°é‡é˜²æ­¢ URL è¿‡é•¿
            safe_kw = kw.replace('"', '').replace("'", "")
            filter_exprs.append(f'section_title like "%{safe_kw}%"')
            filter_exprs.append(f'raw_content like "%{safe_kw}%"')

        if filter_exprs:
            try:
                k_res = self.milvus_client.query(
                    collection_name=COLLECTION_NAME,
                    filter=" or ".join(filter_exprs),
                    limit=RETRIEVE_TOP_K,
                    output_fields=["doc_title", "section_title", "raw_content", "content_type"]
                )
                k_hits = k_res
            except Exception as e:
                logging.error(f"å…³é”®è¯æ£€ç´¢å¤±è´¥: {e}")

        # 5. RRF èåˆ (ç®—æ³•æœ¬èº«ä¸éœ€è¦å˜ï¼Œå› ä¸ºå®ƒå¤©ç„¶æ”¯æŒå»é‡å’Œæ’åº)
        return self.reciprocal_rank_fusion(v_hits, k_hits)

    def reciprocal_rank_fusion(self, v_hits, k_hits):
        scores = {}
        doc_map = {}
        for source_hits in [v_hits, k_hits]:
            for rank, h in enumerate(source_hits):
                # ä½¿ç”¨æ›´çŸ­çš„ hash key èŠ‚çœå†…å­˜
                key = hash(h["raw_content"][:200])
                scores[key] = scores.get(key, 0) + 1.0 / (RRF_K + rank + 1)
                doc_map[key] = h

        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return [doc_map[k] for k, s in sorted_docs[:30]]

    @observe(name="RAG-é‡æ’é˜¶æ®µ")
    def rerank_with_bge(self, query: str, hits: List[Dict]):
        if not hits: return []

        # 1. BGE è¯„åˆ† (Truncate è®¾ä¸º True é˜²æ­¢è¶…é•¿æ–‡æœ¬æŠ¥é”™)
        # BGE-Reranker æœ€å¤§é•¿åº¦æ˜¯ 512 tokenï¼Œè™½ç„¶æˆ‘ä»¬åˆ‡äº†å­—ç¬¦ï¼Œä½†æœ€å¥½æ˜¾å¼æˆªæ–­
        pairs = [[query, h["raw_content"][:1000]] for h in hits]
        bge_scores = self.reranker.predict(pairs)

        q_words = set(self.stop_words_manager.filter_stop_words(query, 1))
        query_lower = query.lower()

        for i, hit in enumerate(hits):
            base = float(bge_scores[i])
            boost = 0.0
            title = hit.get("section_title", "").lower()

            # ç­–ç•¥è®¡ç®—
            t_words = set(self.stop_words_manager.filter_stop_words(title, 1))
            if q_words.intersection(t_words): boost += 25.0
            if ">" in title: boost += 10.0

            # æŠ€æœ¯/ä»£ç æ„å›¾å¥–åŠ±
            if any(term in query_lower for term in ["cert", "init", "done", "log", "å­—æ®µ"]):
                if any(term in title for term in ["base", "script", "reference", "manual"]):
                    boost += 15.0

            if hit.get("content_type") == "code" and \
                    any(k in query_lower for k in ["è„šæœ¬", "ä»£ç ", "å†™", "script", "æ£€æµ‹"]):
                boost += 20.0

            hit["final_score"] = base + boost

        # 2. æ’åº
        sorted_hits = sorted(hits, key=lambda x: x["final_score"], reverse=True)

        # 3. å»é‡
        unique_hits = []
        seen_keys = set()
        for hit in sorted_hits:
            dedup_key = f"{hit.get('doc_title', '')}->{hit.get('section_title', '')}"
            if dedup_key not in seen_keys:
                unique_hits.append(hit)
                seen_keys.add(dedup_key)
            if len(unique_hits) >= RERANK_TOP_K: break

        if unique_hits:
            langfuse.update_current_span(metadata={"top1_score": unique_hits[0]['final_score']})

        return unique_hits

    @observe(name="LLM-ç”Ÿæˆå›ç­”")
    def ask_llm(self, query: str, chunks: List[Dict]):
        if not chunks: return "æœªæ‰¾åˆ°ç›¸å…³å‚è€ƒèµ„æ–™ã€‚"

        context_str = ""
        current_len = 0
        for i, c in enumerate(chunks):
            content = c['raw_content'][:1500] # å•ç‰‡é˜²å¾¡æ€§æˆªæ–­
            chunk_text = f"èµ„æ–™[{i+1}] (æ¥æº: {c['section_title']}):\n{content}\n\n"
            if current_len + len(chunk_text) > MAX_CONTEXT_CHARS: break
            context_str += chunk_text
            current_len += len(chunk_text)

        prompt_name = "zeek_script_coder" if any(k in query for k in ["è„šæœ¬", "ä»£ç ", "å†™"]) else "zeek_rag_qa"

        try:
            langfuse_prompt = langfuse.get_prompt(prompt_name)
            compiled_prompt = langfuse_prompt.compile(context=context_str, query=query)

            # ä½¿ç”¨ session å‘é€
            r = self.session.post(
                f"{OLLAMA_HOST}/api/generate",
                json={
                    "model": LLM_MODEL,
                    "prompt": compiled_prompt,
                    "stream": False,
                    "options": {
                        "temperature": langfuse_prompt.config.get("temperature", 0),
                        "top_p": langfuse_prompt.config.get("top_p", 0.9)
                    }
                },
                timeout=60
            )
            response = r.json().get("response", "ç”Ÿæˆå¤±è´¥")
            langfuse.update_current_span(output=response)
            return response
        except Exception as e:
            logging.error(f"LLM è°ƒç”¨å¼‚å¸¸: {e}")
            return "æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"

    @observe(name="Zeek-RAG-å®Œæ•´é—®ç­”é“¾è·¯")
    def chat(self, query: str):
        print(f"\né—®: {query}")
        langfuse.update_current_span(input=query, metadata={"collection": COLLECTION_NAME})

        hits = self.retrieve(query)
        top = self.rerank_with_bge(query, hits)

        # ä½åˆ†ç†”æ–­
        if not top or top[0]['final_score'] < SCORE_THRESHOLD:
            msg = "æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­ä¼¼ä¹æ²¡æœ‰æ‰¾åˆ°å…³äºæ­¤é—®é¢˜çš„ç›¸å…³ä¿¡æ¯ã€‚"
            print(f"ğŸ¤– Zeek AI:\n{msg}\n{'-'*30}\nâš ï¸ è§¦å‘ä½åˆ†ç†”æ–­")
            langfuse.update_current_span(output=msg, metadata={"status": "rejected"})
            return

        ans = self.ask_llm(query, top)
        print(f"ğŸ¤– Zeek AI:\n{ans}\n{'-'*30}")
        for i, c in enumerate(top):
            print(f"Top {i+1}: {c['section_title']} (Score: {c['final_score']:.2f})")

        langfuse.update_current_span(output=ans)

if __name__ == "__main__":
    assistant = ZeekRAGAssistant()

    # æµ‹è¯•ç”¨ä¾‹
    test_queries = [
        # "å½“å‰zeekç‰ˆæœ¬å·",
        # "what is zeek?",
        # "Zeek æ˜¯ä»€ä¹ˆï¼Ÿ",
        # "ä»‹ç»ä¸€ä¸‹ Zeek çš„æ ¸å¿ƒåŠŸèƒ½",
        # "Zeek å’Œ Suricata çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
        # "zeek_init å’Œ zeek_done å‡½æ•°çš„åŒºåˆ«",
        # "å¦‚ä½•ç”¨ Zeek è„šæœ¬æå– HTTP è¯·æ±‚çš„ URLï¼Ÿ",
        # "Zeek SSL æ—¥å¿—ä¸­çš„ client_cert å­—æ®µå«ä¹‰",
        # "Zeek æ”¯æŒ Python 3.10 å—ï¼Ÿ",
        "å†™ä¸€ä¸ªzeek8.0.4ç‰ˆæœ¬åˆ†æpcapæ–‡ä»¶ä¸­ddosæ”»å‡»çš„zeekè„šæœ¬",
    ]

    for query in test_queries:
        try:
            assistant.chat(query)
            langfuse.flush()    # è®°å¾—åˆ·æ–°æ•°æ®
        except Exception as e:
            logging.error(f"å¤„ç†å¤±è´¥: {e}")